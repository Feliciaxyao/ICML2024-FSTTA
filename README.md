# ICML2024-FSTTA
Fast-Slow Test-time Adaptation for Online Vision-and-Language Navigation
## Introduction
![image](img/FSTTA.png)

### Fast-Slow Test-time Adaptation for Online Vision-and-Language Navigation

Junyu Gao, Xuan Yao, Changsheng Xu

State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences.

[Paper Link on ICML 2024](https://icml.cc/virtual/2024/poster/33723) 

## Usage
Our code will be coming soon!


## Acknowledgements
Our implementations are partially based on [VLN-DUET](https://github.com/cshizhe/VLN-DUET), [HM3DAutoVLN](https://github.com/cshizhe/HM3DAutoVLN) and [VLN-BEVBert](https://github.com/MarSaKi/VLN-BEVBert.git). Thanks to the authors for sharing their code.


## Related Work
* [Reverie: Remote embodied visual referring expression in real indoor environments](https://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_REVERIE_Remote_Embodied_Visual_Referring_Expression_in_Real_Indoor_Environments_CVPR_2020_paper.pdf)
* [Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2004.02857)

## Citation

If you find this project useful in your research, please consider cite:
```
@inproceedings{Gao2024Fast,
  title={Fast-Slow Test-time Adaptation for Online Vision-and-Language Navigation},
  author={Junyu Gao and Xuan Yao and Changsheng Xu},
  journal={Proceedings of the 41st International Conference on Machine Learning},
  year={2024},
  url={}
}
```

If you have any questions, comments or suggestions, please feel free to get in touch with us via the contact information below: junyu.gao@nlpr.ia.ac.cn.
